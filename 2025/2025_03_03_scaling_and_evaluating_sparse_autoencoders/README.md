# Introduction to Sparse Autoencoders with focus on paper: Scaling and evaluating sparse autoencoders

The presentation will focus on foundations of sparse autoencoders as a promising unsupervised approach for extracting interpretable features from a model. I will present top architectures used in this relatively new domain, how they are trained and also evaluated. The presentation will focus on Scaling and evaluating sparse autoencoders paper recently published on ICLR 2025 where authors based on the hypothesis that Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. They proposed k-sparse autoencoder where scaling of size and sparsity improves the reconstruction-sparsity frontier with fewer dead latents and “better” interpretability.

The presentation is based on this [paper](https://openreview.net/forum?id=tcsZt9ZNKD)
