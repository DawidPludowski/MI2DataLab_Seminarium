# Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training

## Abstract

During the previous seminar, we discussed how diffusion models can modify existing images using textual prompts. Following Monday's seminar, we will continue exploring how machine learning models understand the spatial structure of images from the perspective of diffusion models. Our primary focus will be on a 2022 CVPR paper introducing SGDiff, a novel method that leverages graph-structured inputs for image generation by learning scene representation. It trains a separate encoder of such conditioning, relying on two loss functions: masked autoencoding loss and contrastive loss. Later on, we will also touch on other generative methods capable of understanding the spatiality of images.
## Source paper

[Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training](https://arxiv.org/abs/2211.11138)
