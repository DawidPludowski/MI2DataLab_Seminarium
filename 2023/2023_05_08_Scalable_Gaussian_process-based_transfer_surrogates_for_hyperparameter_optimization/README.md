# Scalable Gaussian process-based transfer surrogates for hyperparameter optimization

Hyperparameter optimization is a tedious and time-consuming task, but its importance for obtaining performant machine learning models cannot be overstated. During my presentation, I will introduce the hyperparameter optimization method proposed in the article titled "Scalable Gaussian process-based transfer surrogates for hyperparameter optimization". The considered approach is an extension of the widely used Sequential Model-based optimization, better known as the Bayesian optimization. In short, the base method uses a constantly updated surrogate model and an acquisition function to select the next hyperparameter configuration to test, in order to balance between exploitation of proven and exploration of new optimum candidates. Its extension allows for additional use of knowledge acquired during previous optimization processes, that is observed performances of tested hyperparameter configurations on other data sets. It does so by constructing a weighted ensemble of Gaussian process-based surrogate models. The authors propose two strategies for finding the ensemble weights and, in addition, a way to estimate the acquisition function in a similar manner. They evaluate their ideas on two meta-data sets (data sets of data sets) and report promising results.
